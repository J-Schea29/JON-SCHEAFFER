{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68fdbfd5-d749-441e-9291-48c9e26dfded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5e042b-275d-4a9b-85d8-77d00104f47c",
   "metadata": {},
   "source": [
    "## Imports Needed to run Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95e2ccd0-6df8-4ca5-96b1-58a118dd875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports for data wrangling \n",
    "import cudf\n",
    "import cupy as cp\n",
    "import pandas\n",
    "import numpy\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35ba9ed7-f231-4876-b860-c28fae1a2f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing given data\n",
    "train = cudf.read_csv(\"./input/train.csv\", parse_dates=['date'])\n",
    "\n",
    "test = cudf.read_csv(\"./input/test.csv\", parse_dates=['date'])\n",
    "\n",
    "oil = cudf.read_csv(\"./input/oil.csv\", parse_dates=['date'])\n",
    "\n",
    "holiday = cudf.read_csv(\"./input/holidays_events.csv\")\n",
    "\n",
    "store = cudf.read_csv(\"./input/stores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db416fb9-d8e8-4b59-a7a9-9aae161d54fe",
   "metadata": {},
   "source": [
    "## Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f704e17-0daa-43ac-82b1-83c6b0c916e9",
   "metadata": {},
   "source": [
    "### Capturing Seasonal Holiday Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a05e50e-ddda-4e59-9fa9-b2f39bf7d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting dates to datetime\n",
    "holiday[\"date\"] = cudf.to_datetime(holiday[\"date\"], format='%Y-%m-%d')\n",
    "holiday = holiday.set_index(\"date\")\n",
    "\n",
    "# Keeping only celbrated holidays\n",
    "holiday = holiday.loc[(holiday[\"transferred\"]!=True)].drop(\"transferred\", axis=1)\n",
    "holiday.loc[holiday[\"type\"]==\"Transfer\", \"type\"] = \"Holiday\"\n",
    "\n",
    "# Bridged days are day where there is no work\n",
    "bridge = holiday.loc[holiday[\"type\"]==\"Bridge\"]\n",
    "bridge[\"bridge\"] = True\n",
    "bridge = bridge[[\"bridge\"]]\n",
    "\n",
    "# Special events\n",
    "event = holiday.loc[holiday[\"type\"]==\"Event\"][[\"description\"]]\n",
    "\n",
    "# Keeping only holidays\n",
    "holiday = holiday.loc[holiday[\"type\"]==\"Holiday\"]\n",
    "\n",
    "# Holidays celerbated localy \n",
    "loc_hol = holiday.loc[holiday[\"locale\"]==\"Local\"][[\"locale_name\", \"description\"]]\n",
    "\n",
    "# Holidays celerbrated regionally\n",
    "reg_hol = holiday.loc[holiday[\"locale\"]==\"Regional\"][[\"locale_name\", \"description\"]]\n",
    "\n",
    "#Holidays celberbrated nationally\n",
    "nat_hol = holiday.loc[holiday[\"locale\"]==\"National\"][[\"description\"]]\n",
    "\n",
    "# Recording days Earthquake\n",
    "quake = event.loc[event[\"description\"].str.find(\"Terremoto Manabi\")!=-1]\n",
    "quake[\"time_since_quake\"] = cp.arange(1,len(quake.index)+1)\n",
    "quake.drop(\"description\", axis=1, inplace=True)\n",
    "\n",
    "# Removing Earthquake and adding Sporting Events\n",
    "event = event.loc[event[\"description\"].str.find(\"Terremoto Manabi\")==-1]\n",
    "event.loc[event[\"description\"].str.find(\"futbol\")!=-1, \"description\"]= \"Sports\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8f7c2d-63a5-4f01-bfcb-553480b34fd3",
   "metadata": {},
   "source": [
    "### Location Specific Demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e484d5b1-1842-465e-ae7c-51474ccc5c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure proper format\n",
    "train[\"store_nbr\"] = train[\"store_nbr\"].astype(int)\n",
    "\n",
    "# Merging\n",
    "X = train.merge(store, on=\"store_nbr\", how=\"left\")\n",
    "X.drop(\"cluster\", axis=1, inplace=True)\n",
    "\n",
    "# Converting dates to datetime\n",
    "X[\"date\"] = cudf.to_datetime(X[\"date\"], format='%Y-%m-%d')\n",
    "\n",
    "# Creating feature measuring the total in store promotions.\n",
    "total_other_promo_store = X[[\"date\", \"store_nbr\", \"onpromotion\"]].groupby(['date', 'store_nbr']).sum()[\"onpromotion\"].reset_index()\n",
    "total_other_promo_store = total_other_promo_store.rename(columns={'onpromotion': 'total_other_promo_store',})\n",
    "\n",
    "# Creating feature measuring the total promotions in each town for similar products.\n",
    "total_other_city_promo = X[[\"date\", \"onpromotion\", \"family\", \"city\"]].groupby(['date', 'city', 'family']).sum()[\"onpromotion\"].reset_index()\n",
    "total_other_city_promo = total_other_city_promo.rename(columns={'onpromotion': 'total_other_city_promo',})\n",
    "\n",
    "# Adding new features\n",
    "X = X.merge(total_other_promo_store, on=['date', 'store_nbr'], how=\"left\")\n",
    "X = X.merge(total_other_city_promo, on=['date', 'city', 'family'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4df2e7df-a73d-4a5d-88bb-04e2b33052b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure proper format\n",
    "store[\"store_nbr\"] = store[\"store_nbr\"].astype(int)\n",
    "test[\"store_nbr\"] = test[\"store_nbr\"].astype(int)\n",
    "\n",
    "# Merging\n",
    "X_test = test.merge(store, on=\"store_nbr\", how=\"left\")\n",
    "X_test.drop(\"cluster\", axis=1, inplace=True)\n",
    "\n",
    "# Converting dates to datetime\n",
    "X_test[\"date\"] = cudf.to_datetime(X_test[\"date\"], format='%Y-%m-%d')\n",
    "\n",
    "# Creating feature measuring the total in store promotions.\n",
    "total_other_promo_store = X_test[[\"date\", \"store_nbr\", \"onpromotion\"]].groupby(['date', 'store_nbr']).sum()[\"onpromotion\"].reset_index()\n",
    "total_other_promo_store = total_other_promo_store.rename(columns={'onpromotion': 'total_other_promo_store',})\n",
    "\n",
    "# Creating feature measuring the total promotions in each town for similar products.\n",
    "total_other_city_promo = X_test[[\"date\", \"onpromotion\", \"family\", \"city\"]].groupby(['date', 'city', 'family']).sum()[\"onpromotion\"].reset_index()\n",
    "total_other_city_promo = total_other_city_promo.rename(columns={'onpromotion': 'total_other_city_promo',})\n",
    "\n",
    "# Adding new features\n",
    "X_test = X_test.merge(total_other_promo_store, on=['date', 'store_nbr'], how=\"left\")\n",
    "X_test = X_test.merge(total_other_city_promo, on=['date', 'city', 'family'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d76ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.set_index(\"date\")\n",
    "X_test = X_test.set_index(\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a203c43-19df-48ba-9c9c-f6b423a2dd40",
   "metadata": {},
   "source": [
    "### Merging with Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e576cca1-b3eb-455f-aed1-65a679476c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding national holidays\n",
    "X = X.merge(nat_hol, on=\"date\", how=\"left\")\n",
    "\n",
    "# Bridge days\n",
    "X = X.merge(bridge, on=\"date\", how=\"left\")\n",
    "\n",
    "# Adding local holdays\n",
    "X = X.merge(loc_hol, left_on=[\"date\", \"city\"],\n",
    "            right_on=[\"date\", \"locale_name\"],\n",
    "            suffixes=(None, '_l'), how=\"left\"\n",
    "           )\n",
    "X.drop(\"locale_name\", axis=1, inplace=True)\n",
    "\n",
    "# Adding regional holidays\n",
    "X = X.merge(reg_hol, left_on=[\"date\", \"state\"],\n",
    "            right_on=[\"date\", \"locale_name\"], \n",
    "            suffixes=(None, '_r'),how=\"left\"\n",
    "           )\n",
    "X.drop(\"locale_name\", axis=1, inplace=True)\n",
    "\n",
    "# True if holiday that Day\n",
    "X[\"holiday\"] = (((X[\"descriptionNone\"].isnull()==False) | (X[\"description_l\"].isnull()==False)) | (X[\"description\"].isnull()==False))\n",
    "\n",
    "# Combine Holiday descriptions\n",
    "X.drop(\"descriptionNone\", axis=1, inplace=True)\n",
    "X.drop(\"description_l\", axis=1, inplace=True)\n",
    "X.drop(\"description\", axis=1, inplace=True)\n",
    "\n",
    "#Events\n",
    "X = X.merge(event, on=\"date\", how=\"left\")\n",
    "X = X.rename(columns={'description': 'event',})\n",
    "X[\"event\"] = X[\"event\"].fillna(\"none\")\n",
    "\n",
    "# Adding Quake data\n",
    "X = X.merge(quake, on=\"date\", how=\"left\")\n",
    "X[\"time_since_quake\"] = X[\"time_since_quake\"].fillna(0)\n",
    "\n",
    "#To model a diminishing marginal effect on the economy by the earthquake\n",
    "X[\"time_since_quake_sq\"] = X[\"time_since_quake\"]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e60cf2ae-a8ab-49ee-9b2c-354f2c34131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding national holidays\n",
    "X_test = X_test.merge(nat_hol, on=\"date\", how=\"left\")\n",
    "del nat_hol\n",
    "\n",
    "# Bridge days\n",
    "X_test = X_test.merge(bridge, on=\"date\", how=\"left\")\n",
    "del bridge\n",
    "\n",
    "# Adding local holdays\n",
    "X_test = X_test.merge(loc_hol, left_on=[\"date\", \"city\"],\n",
    "            right_on=[\"date\", \"locale_name\"],\n",
    "            suffixes=(None, '_l'), how=\"left\"\n",
    "           )\n",
    "X_test.drop(\"locale_name\", axis=1, inplace=True)\n",
    "del loc_hol\n",
    "\n",
    "# Adding regional holidays\n",
    "X_test = X_test.merge(reg_hol, left_on=[\"date\", \"state\"],\n",
    "            right_on=[\"date\", \"locale_name\"], \n",
    "            suffixes=(None, '_r'),how=\"left\"\n",
    "           )\n",
    "X_test.drop(\"locale_name\", axis=1, inplace=True)\n",
    "del reg_hol\n",
    "\n",
    "# True if holiday that Day\n",
    "X_test[\"holiday\"] = (((X_test[\"descriptionNone\"].isnull()==False) | (X_test[\"description_l\"].isnull()==False)) | (X_test[\"description\"].isnull()==False))\n",
    "\n",
    "# Combine Holiday descriptions\n",
    "X_test.drop(\"descriptionNone\", axis=1, inplace=True)\n",
    "X_test.drop(\"description_l\", axis=1, inplace=True)\n",
    "X_test.drop(\"description\", axis=1, inplace=True)\n",
    "\n",
    "#Events\n",
    "X_test = X_test.merge(event, on=\"date\", how=\"left\")\n",
    "X_test = X_test.rename(columns={'description': 'event',})\n",
    "X_test[\"event\"] = X_test[\"event\"].fillna(\"none\")\n",
    "del event\n",
    "\n",
    "# Adding Quake data\n",
    "X_test = X_test.merge(quake, on=\"date\", how=\"left\")\n",
    "X_test[\"time_since_quake\"] = X_test[\"time_since_quake\"].fillna(0)\n",
    "del quake\n",
    "\n",
    "#To model a diminishing marginal effect on the economy by the earthquake\n",
    "X_test[\"time_since_quake_sq\"] = X_test[\"time_since_quake\"]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2059c7b0-ce19-4011-929d-cd39e25682df",
   "metadata": {},
   "source": [
    "### Merging with Oil Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9d6f820-485e-4f44-a3ef-821638b194ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "oil[\"date\"] = cudf.to_datetime(oil[\"date\"], format='%Y-%m-%d')\n",
    "oil = oil.set_index(\"date\")\n",
    "X = X.merge(oil, on=\"date\", how=\"left\")\n",
    "X_test = X_test.merge(oil, on=\"date\", how=\"left\")\n",
    "\n",
    "del oil\n",
    "\n",
    "# There is no price of oil on days that the market is closed.\n",
    "# To fill the price, we first fill with the last value.\n",
    "X[\"dcoilwtico\"]= X[\"dcoilwtico\"].ffill()\n",
    "X_test[\"dcoilwtico\"]= X_test[\"dcoilwtico\"].ffill()\n",
    "\n",
    "# We back fill just for first couple values that are empty.\n",
    "X[\"dcoilwtico\"]= X[\"dcoilwtico\"].bfill()\n",
    "X_test[\"dcoilwtico\"]=X_test[\"dcoilwtico\"].bfill()\n",
    "\n",
    "# I just to do a rolling average to smooth out any problems with the empty values,\n",
    "# and to capture any effect of changes. \n",
    "X[\"dcoilwtico\"] = X[\"dcoilwtico\"].rolling(\n",
    "    window=30,       \n",
    "    min_periods=1,  \n",
    ").mean()\n",
    "\n",
    "X_test[\"dcoilwtico\"] = X_test[\"dcoilwtico\"].rolling(\n",
    "    window=30,       \n",
    "    min_periods=1,  \n",
    ").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5cddac-bc83-4fce-903a-ce35d9675dcc",
   "metadata": {},
   "source": [
    "### Time Based Varriables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ef2ed5c-e767-476d-b9cc-50d06717e194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time variables\n",
    "X[\"day\"] = X.index.dayofweek\n",
    "X[\"dayofyear\"] = X.index.dayofyear\n",
    "X[\"month\"] = X.index.month\n",
    "X[\"year\"] = X.index.year\n",
    "\n",
    "# This varible says whether it is a workday.\n",
    "X[\"workday\"] = (((X.bridge.isnull()) & (X.holiday==False)) & ((X[\"day\"]!=5) & (X[\"day\"]!=6)))\n",
    "X.drop(\"bridge\", axis=1, inplace=True)\n",
    "\n",
    "# In Ecudor, people get paid on the 15 and the last day of the month\n",
    "X[\"payday\"] = ((X.index.day==15) | (X.index.day==X.index.to_series().dt.days_in_month)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a7d9e6e-6b89-4dfd-b375-aa0a56cd1270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time variables\n",
    "X_test[\"day\"] = X_test.index.dayofweek\n",
    "X_test[\"dayofyear\"] =X_test.index.dayofyear\n",
    "X_test[\"month\"] = X_test.index.month\n",
    "X_test[\"year\"] = X_test.index.year\n",
    "\n",
    "# This varible says whether it is a workday.\n",
    "X_test[\"workday\"] = (((X_test.bridge.isnull()) & (X_test.holiday==False)) & ((X_test[\"day\"]!=5) & (X_test[\"day\"]!=6)))\n",
    "X_test.drop(\"bridge\", axis=1, inplace=True)\n",
    "\n",
    "# In Ecudor, people get paid on the 15 and the last day of the month\n",
    "X_test[\"payday\"] = ((X_test.index.day==15) | (X_test.index.day==X_test.index.to_series().dt.days_in_month)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad9d618-c5d0-4c41-8d31-e255fdba0d75",
   "metadata": {},
   "source": [
    "### Final Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2deef0a-75d4-4134-9c4e-1f0fb03537c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing data type\n",
    "X_test = X_test.reset_index()\n",
    "X_test = X_test.sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "X_test = X_test.set_index(\"date\")\n",
    "\n",
    "X_test[\"family\"] = X_test[\"family\"].astype('category').cat.codes\n",
    "X_test[\"store_nbr\"] = X_test[\"store_nbr\"].astype('int')\n",
    "X_test[\"holiday\"] = X_test[\"holiday\"].astype('category').cat.codes\n",
    "X_test[\"event\"] = X_test[\"event\"].astype('category').cat.codes\n",
    "X_test[\"city\"] = X_test[\"city\"].astype('category').cat.codes\n",
    "X_test[\"state\"] = X_test[\"state\"].astype('category').cat.codes\n",
    "X_test[\"type\"] = X_test[\"type\"].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "968334b4-6d18-4dd3-b646-d25286f5cccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>type</th>\n",
       "      <th>total_other_promo_store</th>\n",
       "      <th>total_other_city_promo</th>\n",
       "      <th>holiday</th>\n",
       "      <th>event</th>\n",
       "      <th>time_since_quake</th>\n",
       "      <th>time_since_quake_sq</th>\n",
       "      <th>dcoilwtico</th>\n",
       "      <th>day</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>workday</th>\n",
       "      <th>payday</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93.140</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-02</th>\n",
       "      <td>1782</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93.140</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-03</th>\n",
       "      <td>3564</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93.046</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-04</th>\n",
       "      <td>5346</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93.334</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-05</th>\n",
       "      <td>7128</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93.456</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  store_nbr  family  onpromotion  city  state  type  \\\n",
       "date                                                                  \n",
       "2013-01-01     0          1       0            0    18     12     3   \n",
       "2013-01-02  1782          1       0            0    18     12     3   \n",
       "2013-01-03  3564          1       0            0    18     12     3   \n",
       "2013-01-04  5346          1       0            0    18     12     3   \n",
       "2013-01-05  7128          1       0            0    18     12     3   \n",
       "\n",
       "            total_other_promo_store  total_other_city_promo  holiday  event  \\\n",
       "date                                                                          \n",
       "2013-01-01                        0                       0        1      4   \n",
       "2013-01-02                        0                       0        0      4   \n",
       "2013-01-03                        0                       0        0      4   \n",
       "2013-01-04                        0                       0        0      4   \n",
       "2013-01-05                        0                       0        0      4   \n",
       "\n",
       "            time_since_quake  time_since_quake_sq  dcoilwtico  day  dayofyear  \\\n",
       "date                                                                            \n",
       "2013-01-01                 0                    0      93.140    1          1   \n",
       "2013-01-02                 0                    0      93.140    2          2   \n",
       "2013-01-03                 0                    0      93.046    3          3   \n",
       "2013-01-04                 0                    0      93.334    4          4   \n",
       "2013-01-05                 0                    0      93.456    5          5   \n",
       "\n",
       "            month  year  workday  payday  \n",
       "date                                      \n",
       "2013-01-01      1  2013    False   False  \n",
       "2013-01-02      1  2013     True   False  \n",
       "2013-01-03      1  2013     True   False  \n",
       "2013-01-04      1  2013     True   False  \n",
       "2013-01-05      1  2013    False   False  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.reset_index()\n",
    "X = X.sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "X = X.set_index(\"date\")\n",
    "\n",
    "X[\"family\"] = X[\"family\"].astype('category').cat.codes\n",
    "X[\"store_nbr\"] = X[\"store_nbr\"].astype('int')\n",
    "X[\"holiday\"] = X[\"holiday\"].astype('category').cat.codes\n",
    "X[\"event\"] = X[\"event\"].astype('category').cat.codes\n",
    "X[\"city\"] = X[\"city\"].astype('category').cat.codes\n",
    "X[\"state\"] = X[\"state\"].astype('category').cat.codes\n",
    "X[\"type\"] = X[\"type\"].astype('category').cat.codes\n",
    "\n",
    "y = X[[\"store_nbr\", \"family\", \"sales\"]]\n",
    "X.drop(\"sales\", axis=1, inplace=True)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a037ac91-c2d5-4aa7-af0e-875f1a941da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing early time with NaNs\n",
    "X = X.loc[X.index >= \"2015-07-01\"]\n",
    "y = y.loc[y.index >= \"2015-07-01\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705516e1-83e3-42c1-a5be-f4de5165a39f",
   "metadata": {},
   "source": [
    "## Trainning Model\n",
    "###  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b0a9675-4fb0-40b4-b8a7-e47276cb0c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom Time series functions\n",
    "#from jons_time_series_functions import Prepare_data, Hybrid_Time_Series_ML, Hybrid_Pipeline\n",
    "\n",
    "# Data Preprocessing \n",
    "# from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "# from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n",
    "\n",
    "# Cross-Validation\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Models\n",
    "# from sklearn.dummy import DummyRegressor\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "# from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50754687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing \n",
    "from cuml.dask.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from cuml.preprocessing import MinMaxScaler, StandardScaler, SimpleImputer, LabelEncoder, OneHotEncoder\n",
    "from cuml.compose import make_column_transformer\n",
    "from statsmodels.tsa.deterministic import CalendarFourier\n",
    "\n",
    "# Cross-Validation\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Models\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from cuml.linear_model import LinearRegression\n",
    "from cuml.metrics import mean_squared_error, mean_squared_log_error\n",
    "from bayes_opt import BayesianOptimization\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from jons_time_series_functions import DeterministicProcess_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ab88e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available on this device.\n",
      "\n",
      "PyTorch on this device is running version 12.0 Cuda\n",
      "The number of GPUs on this device is 1\n",
      "\n",
      "GPU #1: In our device, we have a NVIDIA GeForce RTX 4070. It\n",
      "has a total memory of 12,878,086,144 and 46 multi-processors.\n"
     ]
    }
   ],
   "source": [
    "#This line is what will tell us if we did our instalation correct\n",
    "print(f'Cuda {\"is\" if torch.cuda.is_available() else \"is not\"} available on this device.')\n",
    "print()\n",
    "\n",
    "# If available, this code will proceed to give the deivice specifications. \n",
    "if torch.cuda.is_available():\n",
    "    \n",
    "    print(f'PyTorch on this device is running version {torch.version.cuda} Cuda')\n",
    "    print(f'The number of GPUs on this device is {torch.cuda.device_count()}')\n",
    "    num = torch.cuda.device_count()\n",
    "    \n",
    "    for i in range(0, num):\n",
    "        print()\n",
    "        print(f'GPU #{i+1}: In our device, we have a {torch.cuda.get_device_name(f\"cuda:{i}\")}. It')\n",
    "        print(f'has a total memory of {\"{:,.0f}\".format(torch.cuda.get_device_properties(f\"cuda:{i}\").total_memory)} and {\"{:,.0f}\".format(torch.cuda.get_device_properties(f\"cuda:{i}\").multi_processor_count)} multi-processors.')\n",
    "        \n",
    "#Defining which device will be used\n",
    "if torch.cuda.is_available(): \n",
    "    dev = \"cuda\" \n",
    "else: \n",
    "    dev = \"cpu\" \n",
    "    \n",
    "#Set device\n",
    "device = torch.device(dev) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bd09e7",
   "metadata": {},
   "source": [
    "### Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "002a6c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing steps\n",
    "numeric_transformer = [['dcoilwtico', 'onpromotion', 'total_other_promo_store', 'total_other_city_promo'], StandardScaler()]\n",
    "categorical_transformer = [['event'], OneHotEncoder(sparse=False, handle_unknown='error', drop=\"first\")]\n",
    "\n",
    "column_list = [\"time_since_quake\", \"time_since_quake_sq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3c2eb7e-bedc-4827-8e94-e7f1f9d4a805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61128339-9284-48a0-a877-867b2bc7e45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColTransformer_gpu:\n",
    "    def __init__(self, transformers):\n",
    "        self.transformers = transformers\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        # Initialize an empty list to store transformed features\n",
    "        X = X.copy()\n",
    "        transformed_features = []\n",
    "        \n",
    "        num = 0\n",
    "        \n",
    "        # Iterate over each transformer and apply fit_transform\n",
    "        for name, transformer in self.transformers:\n",
    "            X_transformed = cudf.DataFrame(transformer.fit_transform(X[name]))\n",
    "\n",
    "            prev_num = num\n",
    "            num += len(X_transformed.columns)\n",
    "            \n",
    "            # Rename columns to ensure uniqueness\n",
    "            X_transformed.columns = [f\"{i}\" for i in range(prev_num, num)]\n",
    "            \n",
    "            transformed_features.append(X_transformed)  # Append transformed feature DataFrame\n",
    "\n",
    "            X  = X.drop(name, axis=1)\n",
    "\n",
    "        transformed_features.append(X)\n",
    "        \n",
    "        # Concatenate transformed features horizontally\n",
    "        X_transformed_concat = cudf.concat(transformed_features, axis=1)\n",
    "        \n",
    "        \n",
    "        return X_transformed_concat\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # Initialize an empty list to store transformed features\n",
    "        transformed_features = []\n",
    "        \n",
    "        num = 0\n",
    "        \n",
    "        # Iterate over each transformer and apply transform\n",
    "        for name, transformer in self.transformers:\n",
    "            X_transformed = cudf.DataFrame(transformer.transform(X[name]))\n",
    "\n",
    "            prev_num = num\n",
    "            num += len(X_transformed.columns)\n",
    "            \n",
    "            # Rename columns to ensure uniqueness\n",
    "            X_transformed.columns = [f\"{i}\" for i in range(prev_num, num)]\n",
    "            \n",
    "            transformed_features.append(X_transformed)  # Append transformed feature DataFrame\n",
    "            \n",
    "            X  = X.drop(name, axis=1)\n",
    "\n",
    "        transformed_features.append(X)\n",
    "        \n",
    "        # Concatenate transformed features horizontally\n",
    "        X_transformed_concat = cudf.concat(transformed_features, axis=1)\n",
    "        \n",
    "        return X_transformed_concat\n",
    "\n",
    "# Define transformers for numerical and categorical features\n",
    "numeric_transformer = ['dcoilwtico', 'onpromotion', 'total_other_promo_store', 'total_other_city_promo'], StandardScaler()\n",
    "categorical_transformer = ['event'], OneHotEncoder(sparse=False, handle_unknown='error', drop=\"first\")\n",
    "\n",
    "# Initialize and apply the ColTransformer_gpu\n",
    "ct = ColTransformer_gpu([numeric_transformer, categorical_transformer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df8565c1-a765-483f-9763-169c7539bfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the preprocessing steps\n",
    "# numeric_transformer = StandardScaler()\n",
    "# categorical_transformer = OneHotEncoder(sparse=False, handle_unknown='error', drop=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "170039e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(X.index.drop_duplicates()) * 2 / 3)\n",
    "test_size = len(X.index.drop_duplicates()) - train_size\n",
    "\n",
    "date_train, date_test = X.index.drop_duplicates()[:train_size], X.index.drop_duplicates()[train_size:]\n",
    "X_train, X_test = X.loc[date_train], X.loc[date_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77b58119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dpg = DeterministicProcess_gpu(1, fourier = [\"W\", 2])\n",
    "# X_1_train = dpg.in_sample(X_train[column_list][~X_train.index.duplicated(keep='last')])\n",
    "# X_1_test = dpg.out_of_sample(X_test[column_list][~X_test.index.duplicated(keep='last')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8553318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dpg = DeterministicProcess_gpu(1, fourier = [\"W\", 2])\n",
    "# X_1_train = dpg.in_sample(X_train[column_list][~X_train.index.duplicated(keep='last')])\n",
    "# X_1_test = dpg.out_of_sample(X_test[column_list][~X_test.index.duplicated(keep='last')])\n",
    "\n",
    "# a=X_train.copy()\n",
    "# X_2_train = a.drop(column_list+[\"id\"], axis=1)\n",
    "# X_2_train = X_2_train.reset_index().drop([\"date\", \"store_nbr\", \"family\", \"state\", \"city\", \"type\", \"dayofyear\", \"year\"], axis=1)\n",
    "\n",
    "# X_2_train = ct.fit_transform(X_2_train)\n",
    "\n",
    "# a=X_test.copy()\n",
    "# X_2_test = a.drop(column_list+[\"id\"], axis=1)\n",
    "# X_2_test = X_2_test.reset_index().drop([\"date\", \"store_nbr\", \"family\", \"state\", \"city\", \"type\", \"dayofyear\", \"year\"], axis=1)\n",
    "\n",
    "# X_2_test = ct.transform(X_2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce6c537e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prepare_data:\n",
    "    '''\n",
    "    A Class for preparing data for Hybrid Models. \n",
    "    \n",
    "    Parameters:\n",
    "        X_1_column: Columns to be used in the 1st Machine Learning model,\n",
    "        unwanted_columns: Columns to not be used for the 2nd Machine Learning Model.\n",
    "                 \n",
    "    Attributes:\n",
    "        X_1(X): Creates data for the first model,\n",
    "        X_2(X): Creates data for the second model,\n",
    "        preprocessor_2: Preprossesing for data of type oj=bject, catergory, and ,\n",
    "        transform(X): Transform data performing X_1(X) and X_2(X)  \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, X_1_column, transformer_list, unwanted_columns=[], to_tensor=False):\n",
    "        '''\n",
    "        Initializes the Prepare_data class.\n",
    "        \n",
    "        Parameters:\n",
    "            X_1_column: Columns to be used in the 1st Machine Learning model,\n",
    "            unwanted_columns: Columns to not be used for the 2nd Machine Learning Model.\n",
    "        '''\n",
    "        \n",
    "        #Defining instance variables\n",
    "        self.column_list = X_1_column\n",
    "        self.unwanted_columns = unwanted_columns\n",
    "        self.to_tensor = to_tensor\n",
    "        self.ct = ColTransformer_gpu(transformer_list)\n",
    "        self.dpg = DeterministicProcess_gpu(1, fourier = [\"W\", 2])\n",
    "        \n",
    "    def X_1_fit_transform(self, X): \n",
    "        '''\n",
    "        Transform data into X_1. Expects linear 1st model so uses Deterministic Process. \n",
    "        \n",
    "        Parameters:\n",
    "            X: Data for the model. \n",
    "        '''\n",
    "        \n",
    "        # return deterministic process\n",
    "        return dpg.in_sample(X[self.column_list][~X.index.duplicated(keep='last')])\n",
    "\n",
    "    def X_1_transform(self, X): \n",
    "        '''\n",
    "        Transform data into X_1. Expects linear 1st model so uses Deterministic Process. \n",
    "        \n",
    "        Parameters:\n",
    "            X: Data for the model. \n",
    "        '''\n",
    "        \n",
    "        return dpg.out_of_sample(X[self.column_list][~X.index.duplicated(keep='last')])\n",
    "\n",
    "    def X_2_fit_transform(self, X):\n",
    "        '''\n",
    "        Transform data into X_2.\n",
    "        \n",
    "        Parameters:\n",
    "            X: Data for the model. \n",
    "        '''\n",
    "        X_2 = X.reset_index()\n",
    "        X_2 = X_2.drop(self.column_list + self.unwanted_columns + [\"date\"], axis=1)\n",
    "        \n",
    "        X_2 = self.ct.fit_transform(X_2)\n",
    "        return X_2\n",
    "        \n",
    "    def X_2_transform(self, X):\n",
    "        '''\n",
    "        Transform data into X_2.\n",
    "        \n",
    "        Parameters:\n",
    "            X: Data for the model. \n",
    "        '''\n",
    "        X_2 = X.reset_index()\n",
    "        X_2 = X_2.drop(self.column_list + self.unwanted_columns + [\"date\"], axis=1)\n",
    "        X_2 = self.ct.transform(X_2)\n",
    "        \n",
    "        return X_2\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        '''\n",
    "        Transform data into X_1 and X_2.\n",
    "        \n",
    "        Parameters:\n",
    "            X: Data for the model. \n",
    "        '''\n",
    "        X_1, X_2 = self.X_1_fit_transform(X), self.X_2_fit_transform(X)\n",
    "        if self.to_tensor:\n",
    "            \n",
    "            X_1 = torch.tensor(X_1.values).to(device)\n",
    "            X_2 = torch.from_numpy(X_2.astype(\"float\")).to(device)\n",
    "        \n",
    "        return X_1, X_2\n",
    "        \n",
    "    def transform(self, X, to_tensor=False):\n",
    "        '''\n",
    "        Transform data into X_1 and X_2.\n",
    "        \n",
    "        Parameters:\n",
    "            X: Data for the model.\n",
    "            to_tensor: If true changes data to tensors on the GPU.\n",
    "        '''\n",
    "        X_1, X_2 = self.X_1_transform(X), self.X_2_transform(X)\n",
    "        \n",
    "        if self.to_tensor:\n",
    "            \n",
    "            X_1 = torch.tensor(X_1.values).to(device)\n",
    "            X_2 = torch.from_numpy(X_2.astype(\"float\")).to(device)\n",
    "            \n",
    "        return X_1, X_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56cfb756",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd = Prepare_data(column_list, [numeric_transformer, categorical_transformer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ba04b33-73ed-4b6e-a81a-63790953d59e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dpg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore_nbr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfamily\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdayofyear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m----> 3\u001b[0m X_1_train, X_2_train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m a2 \u001b[38;5;241m=\u001b[39m X_2_test\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore_nbr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfamily\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdayofyear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m X_1_train, X_2_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mtransform(a2)\n",
      "Cell \u001b[0;32mIn[26], line 86\u001b[0m, in \u001b[0;36mPrepare_data.fit_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m     80\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m    Transform data into X_1 and X_2.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    Parameters:\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m        X: Data for the model. \u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     X_1, X_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_1_fit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_2_fit_transform(X)\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_tensor:\n\u001b[1;32m     89\u001b[0m         X_1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_1\u001b[38;5;241m.\u001b[39mvalues)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[26], line 41\u001b[0m, in \u001b[0;36mPrepare_data.X_1_fit_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03mTransform data into X_1. Expects linear 1st model so uses Deterministic Process. \u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    X: Data for the model. \u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# return deterministic process\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdpg\u001b[49m\u001b[38;5;241m.\u001b[39min_sample(X[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_list][\u001b[38;5;241m~\u001b[39mX\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mduplicated(keep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast\u001b[39m\u001b[38;5;124m'\u001b[39m)])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dpg' is not defined"
     ]
    }
   ],
   "source": [
    "a = X_train.copy().drop([\"id\", \"store_nbr\", \"family\", \"state\", \"city\", \"type\", \"dayofyear\", \"year\"], axis=1).copy()\n",
    "\n",
    "X_1_train, X_2_train = pd.fit_transform(a)\n",
    "\n",
    "a2 = X_2_test.copy().drop([\"id\", \"store_nbr\", \"family\", \"state\", \"city\", \"type\", \"dayofyear\", \"year\"], axis=1)\n",
    "\n",
    "X_1_train, X_2_test = pd.transform(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac6ef3c-551c-4e5c-baeb-04fa9753da40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf280501-8dc3-4496-a7c7-a29767594c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Model: Use the average y value for each catergory from the training data as the predicted y value for the test data.\n",
    "hts = Hybrid_Time_Series_ML(DummyRegressor(strategy=\"mean\"), DummyRegressor(strategy=\"constant\", constant=0))\n",
    "data_prep = Prepare_data(column_list, preprocessor)\n",
    "hp = Hybrid_Pipeline(data_prep, hts)\n",
    "\n",
    "# Use time series split for cross validation. \n",
    "cv_split = TimeSeriesSplit(n_splits = 4)\n",
    "\n",
    "# Create lists to append MSE scores. \n",
    "train_msle = []\n",
    "valid_msle = []\n",
    "\n",
    "# Dates to index through. \n",
    "dates = X.index.drop_duplicates()\n",
    "a = 0\n",
    "# Perform Cross-Validation to determine how model will do on unseen data.\n",
    "for train_index, valid_index in cv_split.split(dates):\n",
    "    a = a+1\n",
    "    print(f\"Fold {a}:\") \n",
    "    model = Hybrid_Pipeline(Prepare_data(column_list, preprocessor), hts)\n",
    "    \n",
    "    # Index dates.\n",
    "    date_train, date_valid = dates[train_index], dates[valid_index]\n",
    "\n",
    "    # Selecting data for y_train and y_valid.\n",
    "    y_train = y.loc[date_train]\n",
    "    y_valid = y.loc[date_valid]\n",
    "\n",
    "    # Selecting data for X_train and X_valid.\n",
    "    X_train = X.loc[date_train]\n",
    "    X_valid = X.loc[date_valid]\n",
    "\n",
    "\n",
    "    # Fitting model.\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Create predictions for Trainning and Validation.\n",
    "    fit = model.predict(X_train)\n",
    "    pred = model.predict(X_valid)\n",
    "    \n",
    "    # MSE for trainning and validation. \n",
    "    train_msle.append(mean_squared_log_error(y_train[\"sales\"], fit))\n",
    "    valid_msle.append(mean_squared_log_error(y_valid[\"sales\"], pred))\n",
    "    \n",
    "    print(f\"Training RMSLE: {cp.sqrt(mean_squared_log_error(y_train.sales, fit)):.3f}, Validation RMSLE: {cp.sqrt(mean_squared_log_error(y_valid.sales, pred)):.3f}\")\n",
    "\n",
    "# Returns the square root of the average of the MSE.\n",
    "print(\"Average Across Folds\")\n",
    "print(f\"Training RMSLE:{cp.sqrt(cp.mean(train_msle)):.3f}, Validation RMSLE: {cp.sqrt(cp.mean(valid_msle)):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5f7392-ec7b-46d4-8031-24b6db335613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Model\n",
    "hp.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd116d2-42fc-489e-8167-20406eb2c7d1",
   "metadata": {},
   "source": [
    "## Final Predictions and Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161b6096-78e0-4052-b58a-398d9c2b54e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Predictions\n",
    "test_id = X_test[[\"family\", \"store_nbr\", \"id\"]]\n",
    "pred = hp.predict(X_test)\n",
    "pred = pred.reset_index()\n",
    "pred = test_id.merge(pred, on=[\"date\", \"store_nbr\", \"family\"])\n",
    "pred = pred[[\"id\", \"sales\"]]\n",
    "pred = pred.set_index(\"id\")\n",
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca14a1b-1fff-4e6e-adec-26c42425112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.to_csv('submission.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a61c910-4a78-462f-ad44-f0c8d196abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api.competition_submit('submission.csv','1st API Submission','store-sales-time-series-forecasting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aacf84-2ee1-47f2-880d-500dc2c23f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (rapidsai)",
   "language": "python",
   "name": "rapidsai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
